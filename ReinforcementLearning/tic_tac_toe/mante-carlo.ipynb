{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 111111\n",
      "[[[ 0.          0.          0.         -3.0264312 ]\n",
      "  [ 0.          0.          0.         -2.25159022]\n",
      "  [ 0.62882     1.8098      0.         -7.29829656]\n",
      "  [-6.99810729 -6.66456366 -7.56846691  0.        ]]\n",
      "\n",
      " [[-3.72378808 -9.05797426  0.          0.        ]\n",
      "  [-8.40467114  4.58       -4.35140927 -9.44374322]\n",
      "  [ 0.          0.          3.122       0.        ]\n",
      "  [ 0.          0.         -5.42464151 -5.88217736]]\n",
      "\n",
      " [[ 0.          0.          0.         -8.95330473]\n",
      "  [-8.56420402  6.2         0.         -8.83700526]\n",
      "  [ 0.          0.         -8.70778362  0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.         -9.70438234 -9.67153593]\n",
      "  [ 0.          0.          0.          8.        ]\n",
      "  [-9.59448881  0.          0.         10.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "episode 111111\n",
      "[[[ 0.          0.          0.         -3.0264312 ]\n",
      "  [ 0.          0.          0.         -1.34282611]\n",
      "  [-0.81138511  1.21931    -1.3906558  -7.29829656]\n",
      "  [-6.99810729 -6.66456366 -7.56846691  0.        ]]\n",
      "\n",
      " [[-3.72378808 -9.05797426  0.          0.        ]\n",
      "  [-8.40467114  4.58       -4.35140927 -9.44374322]\n",
      "  [-3.72378808  0.          3.122       4.58      ]\n",
      "  [ 0.          8.         -1.15132075  0.15891132]]\n",
      "\n",
      " [[ 0.          0.          0.         -8.95330473]\n",
      "  [-8.56420402  6.2         0.         -8.83700526]\n",
      "  [ 0.          0.         -8.70778362  0.        ]\n",
      "  [ 0.         10.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.         -9.70438234 -9.67153593]\n",
      "  [ 0.          0.          0.          8.        ]\n",
      "  [-9.59448881  0.          0.         10.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "episode 111111\n",
      "[[[ 0.          0.          0.         -3.0264312 ]\n",
      "  [ 0.          0.          0.         -1.34282611]\n",
      "  [-0.81138511  1.21931    -1.3906558  -7.29829656]\n",
      "  [-6.99810729 -6.66456366 -7.56846691  0.        ]]\n",
      "\n",
      " [[-3.72378808 -9.05797426  0.          0.        ]\n",
      "  [-8.40467114  4.58       -4.35140927 -9.44374322]\n",
      "  [-3.72378808  0.          3.122       4.58      ]\n",
      "  [ 0.          8.         -1.15132075  0.15891132]]\n",
      "\n",
      " [[ 0.          0.          0.         -8.95330473]\n",
      "  [-8.56420402  6.2         0.         -8.83700526]\n",
      "  [ 0.          8.         -8.70778362  0.        ]\n",
      "  [ 0.         10.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.         -9.70438234 -9.67153593]\n",
      "  [ 0.          0.          0.          8.        ]\n",
      "  [-9.59448881  0.          0.         10.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "episode 111111\n",
      "[[[ 0.          0.          0.         -3.0264312 ]\n",
      "  [ 0.          0.          0.         -1.34282611]\n",
      "  [-0.81138511  1.21931    -1.3906558  -7.29829656]\n",
      "  [-6.99810729 -6.66456366 -7.56846691  0.        ]]\n",
      "\n",
      " [[-3.72378808 -9.05797426  0.          0.        ]\n",
      "  [-8.40467114  4.58       -4.35140927 -9.44374322]\n",
      "  [-3.72378808  0.          3.122       4.58      ]\n",
      "  [ 0.          8.         -1.15132075  0.15891132]]\n",
      "\n",
      " [[ 0.          0.          0.         -8.95330473]\n",
      "  [-8.56420402  6.2         0.         -8.83700526]\n",
      "  [ 0.          8.         -8.70778362  0.        ]\n",
      "  [ 0.         10.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.         -9.70438234 -9.67153593]\n",
      "  [ 0.          0.          0.          8.        ]\n",
      "  [-9.59448881  0.          0.         10.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "episode 111111\n",
      "[[[-5.42464151 -1.3906558  -3.72378808 -6.44958874]\n",
      "  [-9.98452892 -9.82544073 -9.88547166 -4.22693241]\n",
      "  [-0.81138511 -2.51995215 -5.69263068 -7.29829656]\n",
      "  [-6.99810729 -8.08196628 -7.56846691  0.        ]]\n",
      "\n",
      " [[-2.98768915 -4.74601813 -6.29395962 -9.96006644]\n",
      "  [-9.1237839  -2.68781469 -5.50798647 -9.29796003]\n",
      "  [-6.85992777  0.         -3.05747958 -2.23898713]\n",
      "  [-9.54943201 -0.47665237 -3.87196825 -3.02137142]]\n",
      "\n",
      " [[-9.91650884  0.62882     0.         -2.18665237]\n",
      "  [-7.78115566 -1.89895504 -7.56846691 -1.31850263]\n",
      "  [-9.99713318 -0.35389181 -8.25970192  8.        ]\n",
      "  [-9.67153593 10.         -8.83700526  0.        ]]\n",
      "\n",
      " [[ 3.122       0.         -3.94729117 -9.67153593]\n",
      "  [-9.99767787  0.          0.          8.        ]\n",
      "  [-8.8124735  -8.22741238  0.         10.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "episode 111111\n",
      "[[[-7.33080033 -4.02760973 -3.72378808 -7.35045144]\n",
      "  [-9.94072671 -9.87097479 -9.59936745 -5.43469287]\n",
      "  [-3.86789004 -4.37474785 -7.12466098 -8.12580065]\n",
      "  [-7.85294546 -8.24271219 -8.77563841 -8.83700526]]\n",
      "\n",
      " [[-4.3244952  -4.97222592 -8.12900971 -8.12701303]\n",
      "  [-9.2098349  -5.12515978 -5.63271676 -9.53101774]\n",
      "  [-7.89875839  0.         -4.71513912 -4.82492963]\n",
      "  [-9.77201879 -0.47665237 -4.96082928 -4.36719635]]\n",
      "\n",
      " [[-9.95709336  1.87541     1.8098     -2.46657864]\n",
      "  [-8.33535151 -4.50076414 -4.91002856 -4.21230556]\n",
      "  [-9.99713318 -0.35389181 -8.25970192 -0.99995078]\n",
      "  [-9.83571328 10.         -8.83700526  0.        ]]\n",
      "\n",
      " [[ 1.87541    -9.59448881 -2.77621478 -2.54576797]\n",
      "  [-9.99767787  6.2        -9.67153593  8.        ]\n",
      "  [-8.8124735  -8.22741238  0.         10.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "episode 111111\n",
      "[[[-5.89601062 -4.02760973 -3.72378808 -6.07573614]\n",
      "  [-9.94072671 -7.04420179 -9.59936745 -5.43469287]\n",
      "  [-3.86789004 -4.37474785 -7.12466098 -8.12580065]\n",
      "  [-7.85294546 -8.24271219 -8.77563841 -8.83700526]]\n",
      "\n",
      " [[-4.3244952  -4.97222592 -8.12900971 -5.20840202]\n",
      "  [-9.2098349  -3.39141984 -4.33305307 -9.53101774]\n",
      "  [-7.89875839  0.         -4.71513912 -2.06869723]\n",
      "  [-9.77201879  2.34889842 -4.96082928 -4.36719635]]\n",
      "\n",
      " [[-9.95709336  1.87541     1.8098     -2.46657864]\n",
      "  [-8.33535151 -4.50076414 -4.91002856 -2.37872917]\n",
      "  [-2.70856659 -0.35389181 -8.25970192 -0.99995078]\n",
      "  [-9.83571328 10.         -8.83700526  0.        ]]\n",
      "\n",
      " [[ 1.87541    -9.59448881 -2.77621478 -2.54576797]\n",
      "  [-9.99767787  6.2        -9.67153593  8.        ]\n",
      "  [-8.8124735  -8.22741238  0.         10.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "episode 111111\n",
      "[[[-5.89601062 -5.83298756 -3.72378808 -6.72324156]\n",
      "  [-9.61158605 -7.49240266 -9.5660346  -5.43469287]\n",
      "  [-3.86789004 -4.37474785 -7.12466098 -5.88075237]\n",
      "  [-7.85294546 -6.29054965 -8.77563841 -8.83700526]]\n",
      "\n",
      " [[-5.58885563 -6.15526503 -8.12900971 -6.40099593]\n",
      "  [-9.19542039 -4.70327573 -5.41323128 -9.24943109]\n",
      "  [-6.48696635  0.         -5.67740534 -3.30044026]\n",
      "  [-8.15343531  3.31167382 -4.57394966 -4.2385147 ]]\n",
      "\n",
      " [[-9.84974221  1.87541    -3.93086797 -4.31458573]\n",
      "  [-8.69165106 -5.83629727 -6.56426781 -3.86419238]\n",
      "  [-2.70856659 -0.35389181 -8.76796619  0.37403281]\n",
      "  [-5.03047552 10.         -3.51360263  8.        ]]\n",
      "\n",
      " [[ 1.87541    -9.59448881 -2.77621478 -2.54576797]\n",
      "  [-9.9115593   6.2        -9.67153593  8.        ]\n",
      "  [-8.8124735  -8.22741238  0.         10.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "episode 111111\n",
      "[[[-5.89601062 -5.83298756 -3.72378808 -6.72324156]\n",
      "  [-9.61158605 -7.49240266 -9.5660346  -5.43469287]\n",
      "  [-3.86789004 -2.58379828 -7.12466098 -5.88075237]\n",
      "  [-7.85294546 -6.29054965 -4.80975894 -8.83700526]]\n",
      "\n",
      " [[-5.58885563 -6.15526503 -8.12900971 -6.40099593]\n",
      "  [-9.19542039 -4.70327573 -5.41323128 -9.24943109]\n",
      "  [-6.48696635  0.         -5.67740534 -1.71703355]\n",
      "  [-8.15343531  4.24933905 -4.57394966 -4.2385147 ]]\n",
      "\n",
      " [[-9.84974221  1.87541    -3.93086797 -4.31458573]\n",
      "  [-8.69165106 -5.83629727 -6.56426781 -3.86419238]\n",
      "  [-2.70856659 -0.35389181 -8.76796619  0.37403281]\n",
      "  [-5.03047552 10.         -3.51360263  8.        ]]\n",
      "\n",
      " [[ 1.87541    -9.59448881 -2.77621478 -2.54576797]\n",
      "  [-9.9115593   6.2        -9.67153593  8.        ]\n",
      "  [-8.8124735  -8.22741238  0.         10.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "episode 111111\n",
      "[[[-5.89601062 -5.83298756 -3.72378808 -6.72324156]\n",
      "  [-9.61158605 -7.49240266 -9.5660346  -5.43469287]\n",
      "  [-3.86789004 -2.58379828 -7.12466098 -5.88075237]\n",
      "  [-7.85294546 -6.29054965 -4.80975894 -8.83700526]]\n",
      "\n",
      " [[-5.58885563 -6.15526503 -8.12900971 -6.40099593]\n",
      "  [-9.19542039 -4.70327573 -5.41323128 -9.24943109]\n",
      "  [-6.48696635  0.         -5.67740534 -1.71703355]\n",
      "  [-8.15343531  4.24933905 -4.57394966 -4.2385147 ]]\n",
      "\n",
      " [[-9.84974221 -1.0824291  -3.93086797 -4.31458573]\n",
      "  [-8.69165106 -5.83629727 -6.747775   -2.18682699]\n",
      "  [-2.70856659  2.43073879 -8.76796619  0.37403281]\n",
      "  [-5.03047552 10.         -3.51360263  8.        ]]\n",
      "\n",
      " [[ 1.87541    -8.91095059 -4.08977563 -3.91869986]\n",
      "  [-5.08103953  4.661      -9.03810354  5.54294   ]\n",
      "  [-8.8124735  -6.28941082  1.8098     10.        ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "2 [-5.89601062 -5.83298756 -3.72378808 -6.72324156]\n",
      "3 [-9.61158605 -7.49240266 -9.5660346  -5.43469287]\n",
      "1 [-3.86789004 -2.58379828 -7.12466098 -5.88075237]\n",
      "2 [-7.85294546 -6.29054965 -4.80975894 -8.83700526]\n",
      "0 [-5.58885563 -6.15526503 -8.12900971 -6.40099593]\n",
      "1 [-9.19542039 -4.70327573 -5.41323128 -9.24943109]\n",
      "1 [-6.48696635  0.         -5.67740534 -1.71703355]\n",
      "1 [-8.15343531  4.24933905 -4.57394966 -4.2385147 ]\n",
      "1 [-9.84974221 -1.0824291  -3.93086797 -4.31458573]\n",
      "3 [-8.69165106 -5.83629727 -6.747775   -2.18682699]\n",
      "1 [-2.70856659  2.43073879 -8.76796619  0.37403281]\n",
      "1 [-5.03047552 10.         -3.51360263  8.        ]\n",
      "0 [ 1.87541    -8.91095059 -4.08977563 -3.91869986]\n",
      "3 [-5.08103953  4.661      -9.03810354  5.54294   ]\n",
      "3 [-8.8124735  -6.28941082  1.8098     10.        ]\n",
      "Learned Policy:\n",
      "[['L' 'R' 'D' 'L']\n",
      " ['U' 'D' 'D' 'D']\n",
      " ['D' 'R' 'D' 'D']\n",
      " ['U' 'R' 'R' 'G']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "grid_size = 4\n",
    "goal_state = (3, 3)\n",
    "actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "action_dict = {\"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)}\n",
    "\n",
    "# Initialize Q-table and returns dictionary\n",
    "Q = np.zeros((grid_size, grid_size, len(actions)))\n",
    "returns = {}  # Stores cumulative returns for state-action pairs\n",
    "gamma = 0.9  # Discount factor\n",
    "alpha = 0.1  # Learning rate\n",
    "episodes = 10  # Number of episodes\n",
    "\n",
    "# Reward function\n",
    "def get_reward(state, next_state):\n",
    "    if next_state == goal_state:\n",
    "        return 10\n",
    "    elif next_state[0] < 0 or next_state[0] >= grid_size or next_state[1] < 0 or next_state[1] >= grid_size:\n",
    "        return -5  # Hitting a wall\n",
    "    else:\n",
    "        return -1  # Regular step cost\n",
    "\n",
    "# Generate an episode (returns a list of (state, action, reward))\n",
    "def generate_episode():\n",
    "    episode = []\n",
    "    state = (random.randint(0, grid_size - 1), random.randint(0, grid_size - 1))  # Start from random state\n",
    "    \n",
    "    while state != goal_state:\n",
    "      \n",
    "        action = random.choice(actions)  # Explore randomly\n",
    "        next_state = (state[0] + action_dict[action][0], state[1] + action_dict[action][1])\n",
    "        # print(state,action,next_state)\n",
    "        # Ensure state is within bounds\n",
    "        if next_state[0] < 0 or next_state[0] >= grid_size or next_state[1] < 0 or next_state[1] >= grid_size:\n",
    "            next_state = state  # Stay in place if hitting a wall\n",
    "        \n",
    "        reward = get_reward(state, next_state)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state  # Move to the next state\n",
    "    \n",
    "    return episode\n",
    "\n",
    "# Monte Carlo learning\n",
    "for episode_num in range(episodes):\n",
    "    print('episode 111111')\n",
    "    episode = generate_episode()\n",
    "    # print(len(episode),episode)\n",
    "    G = 0  # Initialize return\n",
    "    visited = set()  # Track first visits\n",
    "\n",
    "    for t in reversed(range(len(episode))):  # Work backwards from end to start\n",
    "        state, action, reward = episode[t]\n",
    "        G = reward + gamma * G  # Compute return\n",
    "\n",
    "        if (state, action) not in visited:  # First-visit MC\n",
    "            visited.add((state, action))\n",
    "            i, j = state\n",
    "            a_index = actions.index(action)\n",
    "            \n",
    "            # Update returns dictionary\n",
    "            if (i, j, a_index) not in returns:\n",
    "                returns[(i, j, a_index)] = []\n",
    "            returns[(i, j, a_index)].append(G)\n",
    "            # print(returns)\n",
    "            # Update Q-value with the mean return\n",
    "            Q[i, j, a_index] = np.mean(returns[(i, j, a_index)])\n",
    "    print(Q)\n",
    "# Derive policy from Q-values\n",
    "policy = np.chararray((grid_size, grid_size), unicode=True)\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        if (i, j) == goal_state:\n",
    "            policy[i, j] = \"G\"\n",
    "        else:\n",
    "            best_action = np.argmax(Q[i, j])\n",
    "            print(np.argmax(Q[i,j]),Q[i,j])\n",
    "            policy[i, j] = actions[best_action][0].upper()  # First letter of the best action\n",
    "\n",
    "print(\"Learned Policy:\")\n",
    "print(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Policy:\n",
      "[['D' 'R' 'D' 'D']\n",
      " ['R' 'R' 'R' 'D']\n",
      " ['R' 'R' 'R' 'D']\n",
      " ['R' 'R' 'R' 'G']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the Gridworld size\n",
    "grid_size = 4\n",
    "goal_state = (3, 3)\n",
    "actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "action_dict = {\"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)}\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "Q = np.zeros((grid_size, grid_size, len(actions)))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration probability\n",
    "episodes = 10000  # Number of episodes\n",
    "\n",
    "# Reward function\n",
    "def get_reward(state, next_state):\n",
    "    if next_state == goal_state:\n",
    "        return 10\n",
    "    elif next_state[0] < 0 or next_state[0] >= grid_size or next_state[1] < 0 or next_state[1] >= grid_size:\n",
    "        return -5  # Hitting a wall\n",
    "    else:\n",
    "        return -1  # Regular step cost\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(episodes):\n",
    "    state = (0, 0)  # Start from top-left corner\n",
    "    \n",
    "    while state != goal_state:\n",
    "        # Choose action using epsilon-greedy\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(actions)  # Exploration\n",
    "        else:\n",
    "            action = actions[np.argmax(Q[state[0], state[1]])]  # Exploitation\n",
    "\n",
    "        # Perform action\n",
    "        next_state = (state[0] + action_dict[action][0], state[1] + action_dict[action][1])\n",
    "\n",
    "        # Stay in place if hitting a wall\n",
    "        if next_state[0] < 0 or next_state[0] >= grid_size or next_state[1] < 0 or next_state[1] >= grid_size:\n",
    "            next_state = state\n",
    "\n",
    "        # Get reward\n",
    "        reward = get_reward(state, next_state)\n",
    "\n",
    "        # Q-learning update\n",
    "        Q[state[0], state[1], actions.index(action)] += alpha * (\n",
    "            reward + gamma * np.max(Q[next_state[0], next_state[1]]) - Q[state[0], state[1], actions.index(action)]\n",
    "        )\n",
    "\n",
    "        state = next_state  # Move to the next state\n",
    "\n",
    "# Extract learned policy\n",
    "policy = np.chararray((grid_size, grid_size), unicode=True)\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        if (i, j) == goal_state:\n",
    "            policy[i, j] = \"G\"\n",
    "        else:\n",
    "            best_action = np.argmax(Q[i, j])\n",
    "            policy[i, j] = actions[best_action][0].upper()  # First letter of best action\n",
    "\n",
    "# Print the learned policy\n",
    "print(\"Learned Policy:\")\n",
    "print(policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning vs. Monte Carlo\n",
    "\n",
    "\n",
    "Feature\tQ-learning\tMonte Carlo\n",
    "\n",
    "Update Timing\tAfter each step\tAfter each episode\n",
    "\n",
    "Exploration\tEpsilon-greedy\tStarts randomly\n",
    "\n",
    "Continuous Tasks\tYes\tNo\n",
    "\n",
    "Convergence Speed\tFaster\tSlower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
